# 5. 레디스를 캐시로 사용하기

다음 조건을 만족할때 cahce도입시 성능을 개선할 수 있다.
- 원본 데이터를 찾기 위한 검색시간이 오래걸리거나, 매번 계산해야 하는 경우
- 캐시요청시 원본 저장소보다 빠른경우
- 캐시에 저장된 데이터는 잘 변하지 않는 경우
- 캐시에 저장된 데이터가 자주 검색되는 경우

> Q) 3번 항목의 잘 변하지 않는 경우? 과연 몇분? 몇시간? 얼마나 안바뀌어야 잘 변하지 않는다는 것 일까? 상대적 기준같다 생각하는데... 음...

RDB와 달리 모든 데이터를 메모리에서 접근하기 때문에, 평균 읽기 쓰기 속도가 1ms 미만

## 5-1. 캐싱 전략
캐싱 전략은 캐싱되는 데이터의 유형과 데이터에 대한 엑세스 패턴에 따라 다르기 때문에 서비스에 맞는 적절한 캐싱 전략을 선택하는 것이 중요하다.

### 5-1-1) 읽기 전략 - look aside
가장 일반적으로 사용하는 전략

1. 레디스에 데이터가 있다면 레디스에서 조회
2. 없다면 DB에 조회

기존 애플리케이션이 Redis를 통해 데이터를 가져오는 연결이 매우 많았다면 모든 커넥션이 한번에 원본 DB로 몰려 많은 부하를 발생시키고, 이로 인해 원본 DB의 응답이 느려지거나 리소스를 많이 차지하는 등의 이슈가 발생

초기 앱 실행시 아직 Redis에 캐싱된 데이터가 없어 이를 가져오는데 시간이듬
-> 이를 해결하기 위해 미리 DB에서 캐시로 데이터를 추가하는 작업은 `Cache-warming`이라 부른다.

### 5-1-2) 쓰기 전략과 캐시의 일관성
1. write through
   DB에 업데이트 할때마다 매번 캐시에도 데이터를 함께 업데이트 시키는 방식
   항상 캐시는 최신의 데이터를 갖지만, 데이터를 매번 2개의 저장소에 저장되어야 하기 때문에 데이터를 쓸때 시간이 많이 걸릴 수 있다.

2. cache invalidation
   DB에 값을 업데이트 할 때마다 cache에서는 데이터를 삭제하는 전략

3. write behind
   write가 빈번한 서비스라면 이 방식을 고려해볼 수 있다.
   대량의 쓰기 작업이 발생하면 이는 많은 I/O를 유발해 성능 저하가 발생할 수 있다.
   따라서 먼저 데이터를 빠르게 접근할 수 있는 Cache에 업데이트 한 뒤, 이후에는 비동기적으로 DB에 업데이트 하는 방식

## 5-2. 캐시에서 데이터 흐름
Redis는 데이터가 가득 차지 않도록 일정 양의 데이터를 유지해야 하며, 계속 새로운 데이터가 저장되고 삭제될 수 있도록 관리돼야 한다.

### 5-2-1) 만료시간 (TTL)
`EXPIRE` 커맨드를 사용하면 만료 시간을 설정할 수 있다.
`TTL` 커맨드를 사용하면 키에 대한 만료 시간을 확인할 수 있으며, 남아 있는 시간이 있다면 남은 시간을 반환한다. 키가 존재하지 않는 경우 (-2), 해당 키에 만료시간이 없는 경우(-1) 반환

밀리세컨드 단위는 `PTTL`, `PEXPIRE` 를 사용

기존 키에 새로운 값을 저장해 키를 덮어 쓰는 경우 이전 만료시간을 사라진다.

## 키가 만료됐다 해서 바로 메모리에서 삭제되는 것은 아니다


## 5-3. 메모리 관리와 maxmemory-policy 설정
Redis에서 키에 만료 시간을 설정해 자동으로 삭제되도록 함으로써 데이터의 수명을 관리할 수 있지만,
메모리가 제한적이라 너무 많은 키가 저장되면 메모리가 가득 차는 상황이 발생.
-> 이런 경우 Redis는 내부 정책을 통해 어떤 키를 삭제할지 결정

최대 용량 설정 : maxmemory 설정
용량 초과시 처리 방식 셜정 : maxmemory-policy

### 5-3-1) Noeviction
데이터가 다 차면 임의로 삭제하는 것 이 아니라, 예외를 던지는 방식
-> 권장하지 않는 방식, 장애로 이어질수 있기 때문

### 5-3-2) LRU eviction
가장 최근에 사용되지 않는 데이터부터 삭제하는 정책
- volatile-lru : 만료시간이 설정된 키에 한하여 LRU 방식으로 키를 삭제
- allkeys-lru : 이방식 권장. 모든 키에 대하여 LRU 알고리듬을 이용해 삭제하기 때문에 메모리가 가득차 장애가 발생하는 일은 방지할 수 있다.

### 5-3-3) LFU eviction
가장 오래된 시간이 아니라, 가장 사용 빈도수가 적은것을 먼저 삭제하는 방식
- volatile-lfu : 만료시간이 설정된 키에 한하여 LFU 방식으로 키를 삭제
- allkeys-lfu : 모든 키에 대하여 LFU 알고리듬으로 삭제

> Q) 아니 그럼 평상시에 얼마나 사용하지는지를 다 저장하고 있다는건가??
> NOTE에 보니 근사 알고리즘을 사용한다 하는데?? 어떤 근사 알고리즘? 어떻게 동작할까?

**Redis****에서는 광범위하게 다루고 있는 Approximated LRU algorithm****

Redis LRU 및 LFU eviction 정책은 근사값이라는 점에 유의해야 한다.

Redis에서의 LRU 알고리즘은 정확하게 구현되어 있지 않다. 즉, Redis가 eviction을 위해 가장 오랫동안 사용되지 않은 최상위 후보를 선택할 수 없음을 의미한다.

대신에, 소수의 keys를 샘플링하고, 샘플링된 key 중에서 가장 좋은(가장 오래된 엑세스 Time을 가진) key를 eviction하기 위해 LRU approximation 알고리즘을 실행할 것 이다.

Redis 3.0부터 알고리즘은 더 좋은 eviction 후보풀을 얻기 위해 개선되었다. 이로 인해 알고리즘의 성능이 향상되어 실제 LRU 알고리즘의 동작에 더 근사할 수 있게 되었다.

Redis에서는 maxmemory-samples 파라미터 설정에서 지정한 수의 key로 샘플링하여 LRU 알고리즘의 근사치를 계산할 수 있다. 샘플 수가 많을수록 메모리는 많이 사용되지만 정밀도가 올라가는 특징이 있다.

다음과 같이 해당 파라미터는 redis.conf에서 조정할 수 있다.

![51_7](https://github.com/TReKa-Genius-Library/redis-for-developer/assets/60593969/720d81ca-640d-4ce3-989c-18844af4ce97)

Redis가 실제 LRU를 사용하지 않는 이유는 더 많은 메모리가 필요하고 CPU 사용량이 증가하기 때문이다.

아래의 그림은 Redis에서 사용하는 LRU approximation과 실제 LRU를 비교한 것이다.
![LRU approximation](https://github.com/TReKa-Genius-Library/redis-for-developer/assets/60593969/dffdd606-01d6-44a1-9c03-d6fd7feea752)

위의 그래프에서 Redis는 실제 LRU와 근사치 LRU로 구분되어 있는데, Redis 3.0에서는 eviction을 위한 best candidates을 선택하는 알고리즘이 개선되면서, 실제 LRU 알고리즘과 더 유사해졌다. Redis 3.0에서 샘플링 수를 10으로 설정하면 좀 더 실제에 가까운 LRU 기능과 유사해질 수 있다.

단, 샘플 개수를 늘리면 실제 LRU 알고리즘과 더 유사해진다. 하지만 적정 수준 이상으로 샘플 개수를 설정하면 Redis의 CPU 사용량이 늘어나고 응답 속도가 늦어지는 리스크가 있다는 점을 고려해야 할 것이다.

### 5-3-4) Random eviction
렌덤하게 삭제하는 방식, 권장하지 않는 방식이다.

### 5-3-3) volatile-ttl
만료시간이 가장 작은 키를 삭제하는 방식. 즉, 삭제 예정 시간이 얼마 남지 않은 키를 추출해서 해당 키를 미리 삭제하는 옵션이다.

## 5-4. 캐시 스탬피드 현상
> Q) 처음 들어본 용어, 스탬피드??

모든 키에 대하여 만료시간 설정을 권장하지만, 만료시간을 어떻게 설정하느냐에 따라 캐시 스탬피드와 같은 예상치 못한 문제가 발생한다.

요약해보면, 하나의 Cache서버에 2개 이상의 app이 요청을 보낼경우
만약 데이터가 캐싱되어 있지 않다면 RDB에 `중복 읽기` 발생후 Redis에 `중복 쓰기` 가 발생한다.

이러한 문제는 만료시간을 너무 짧지 않게 설정해주면 줄일 수 있다.

### 5-4-1) 선 계산
동시 다발적으로 여러 app들이 데이터가 만료될 시간에 접근해오는 것 이 문제.
키가 만료되기 전에 이 값을 미리 갱신한다면 해결 가능

### 5-4-2) PER 알고리듬
스탬피드 현상을 완화시키는 확률적 조기 재계산 알고리듬.
캐시 값이 만료되기 전에 언제 데이터베이스에 접근해서 값을 읽어오면 되는지 최적으로 계산할 수 있다.

## 5-5) 세션 스토어로서의 Redis
흔히 발생하는 여러 서버간의 세션 정보의 동기화 문제를 해결해줄 수 있다.
문제들 : 한 서버에 고정되는 sticky session, all-to-all 방식 등
